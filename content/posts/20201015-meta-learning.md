+++
author = "Brian Tang"
title = "HAI 20 Meta Learning"
date = "2020-10-15"
tags = [
    "hai20",
    "metalearning",
    "agi",
    "neuroscience",
    "psychology"
]
categories = [
    "machinelearning",
]
+++
## High level takeaways
- Training and testing on fixed distributions should be changed to distributions on distributions (meta learning)
<!--more-->
- Transformer architectures can do relational representations (usually used for natural language but can be extended to object detection)
- Object based representation of relations is useful for learning physics and continuity
- Complicated images take longer for the brain to process and solve
- Intuitive physics and graphs

### Modeling things requires the following parameters:
1. Architecture
2. Task
3. Dataset
4. Learning rule

## Things to look into
- ConvRNN (tradeoff with the number of neurons and the performance)
- Deep constrastive embeddings
- Beta variational auto encoder

## Links
- [HAI 20 HRI]({{<ref"/20201015-hri-learning">}})